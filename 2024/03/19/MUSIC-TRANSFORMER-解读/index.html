<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>MUSIC TRANSFORMER 解读 | 当兵的纸兔子</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
    <meta name="description" content="只要是艺术，无论什么都感兴趣！！">
  
  
  
    <link rel="alternate" href="/atom.xml" title="当兵的纸兔子" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">当兵的纸兔子</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">多才多艺而非不顾正业</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/."><i class="fa fa-home"></i> Home</a>
        
          <a class="main-nav-link" href="/archives/"><i class="fa fa-archive"></i> Archive</a>
        
          <a class="main-nav-link" href="/about/"><i class="fa fa-user"></i> About</a>
        
      </nav>
    </div>
    <div id="search-form">
      <div id="result-mask" class="hide"></div>
      <label><input id="search-key" type="text" autocomplete="off" placeholder="search"></label>
      <div id="result-wrap" class="hide">
        <div id="search-result"></div>
      </div>
      <div class="hide">
        <template id="search-tpl">
          <div class="item">
            <a href="/{path}" title="{title}">
              <div class="title">{title}</div>
              <div class="time">{date}</div>
              <div class="tags">{tags}</div>
            </a>
          </div>
        </template>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-MUSIC-TRANSFORMER-解读" class="h-entry article article-type-post"
  itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-inner">
    
      
        <header class="article-header">
          
  
    <h1 class="p-name article-title" itemprop="headline name">
      MUSIC TRANSFORMER 解读
    </h1>
  


        </header>
        
          <div class="article-meta">
            
                <span class="article-date">
  <i class="fa fa-date"></i>
  <time class="dt-published" datetime="2024-03-19T10:00:03.000Z" itemprop="datePublished">2024年03月19日</time>
</span>
                  
                    
                      <span class="article-views">
  <i class="fa fa-views"></i>
  <i id="busuanzi_container_page_pv">
      <i id="busuanzi_value_page_pv"></i>
  </i>
</span>

                        
                          
<a href="/2024/03/19/MUSIC-TRANSFORMER-%E8%A7%A3%E8%AF%BB/#comments" class="article-comment-link">
  
    
    
    
    
    
  
  <i class="fa fa-commt"></i>
  Guestbook
</a>


          </div>
          <div class="e-content article-entry" itemprop="articleBody">
            
                  <h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>通过对Transformer的学习和了解之后，进一步对Transformer对音乐生成的方法进行学习了解，本文算是对文章MUSIC TRANSFORMER:GENERATING MUSIC WITH LONG-TERM STRUCTURE的记录笔记。</p>
<p>参考博客：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/542175873">Music Transformer：生成具有宏观结构的音乐 - 知乎 (zhihu.com)</a></p>
<h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>音乐创作的过程中，相对时机的掌握至关重要，现有的Transformer中表示相对位置信息的方法是根据成对的距离调节注意力[[1]](<a target="_blank" rel="noopener" href="https://magenta.tensorflow.org/2016/07/15/lookback-rnn-attention-rnn">Generating Long-Term Structure in Songs and Stories (tensorflow.org)</a>)，而文章出了一种算法，将原来中间相关信息记忆复杂性从是序列的二次方减少到与序列线性相关</p>
<span id="more"></span>

<h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>音乐具有两大重要维度：时间（timing）和音调（pitch）</p>
<p>与传统的Transformer模型相比，本文提出的方法可以维护数据集中存在的有规律的时序网络，并且可以捕捉全部的时间片段，生产常规语句。</p>
<p>本文将展示算法如何将原始的相对注意力算法中需要的序列$O(L^2D)$优化减少为$O(LD)$，并对数据集中的一些数据进行限制处理，对所提出的算法进行优化。</p>
<h4 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h4><ul>
<li><p>对于J.S.BACH CHORALES数据集处理：</p>
<p>通过语言建模的方式训练音乐生成模型，将音乐表示为一系列离散标记。不同流派的音乐风格通过不同的方式记录成离散数据。将数据集信息表示为矩阵，行对应的是时间序列，列对应的是相对于时间离散的十六分音符。通过先向下以一列再向右移的光栅扫描方式序列化。</p>
<p>如下图所示，左图中的行便是四个声部MIDI的音调（音乐设备数字接口）编号。从上到下分别是女高音 (S)、中音 (A)、次中音 (T) 和低音 (B)，而列是以相对时间离散的十六分音符，得到的序列是 S1A1T1B1S2A2T2B2…</p>
</li>
</ul>
<center>
<div style="display:inline-block"><img src="/../img/Trm/9e309898d20d780778f8bc5520d06ae.png" width="800" title="JSB数据集处理"></div>
</center>

<ul>
<li><p>对于PIANO-E-COMPETITION数据集处理：</p>
<p>相比于JSB数据集的数据，钢琴演奏数据包含了更多声音信息，因此使用Oore等人提出的演奏编码方式。</p>
<p>如下图所示，对输入 MIDI 文件进行预处理，以根据延音踏板控制事件延长音符持续时间。每当遇到值 &gt;&#x3D; 64 的延音控制变化时，延音踏板就被认为是踩下的；然后，在控制更改值 &lt; 64 后，延音踏板被视为没被踩下。在延音踏板踩下的时间段内，每个音符的持续时间会延长到相同音高的下一个音符的开头或下一个音符的结尾。</p>
<p>随后，MIDI 音符事件被转换为来自以下词汇集的序列： 128 个 NOTE_ON 事件，用于标记音符开始； 128 个 NOTE_OFF 事件，用于标记音符结束； 100 个TIME_SHIFT 事件表示以 10ms 为增量的时间，32 个 SET_VELOCITY 事件表示未来 NOTICE_ON 事件的速度，其形式为量化到 32 个 bin 的 128 种可能的 MIDI 速度。</p>
<p>数据由上到下进行序列化记录。</p>
</li>
</ul>
<center>
<div style="display:inline-block"><img src="/../img/Trm/d6db40d9d48b7b5de19abd16713abae.png" width="800" title="PIANO-E-COMPETITION数据集处理"></div>
</center>

<ul>
<li><p>Transformer和相对位置自注意力机制</p>
<p>Transformer核心公式，有关Transformer整理[点此](<a target="_blank" rel="noopener" href="https://rainbit-ye.github.io/2024/03/10/Transformer%E5%88%9D%E6%AD%A5%E4%BA%86%E8%A7%A3/#more">Transformer初步了解 | 当兵的纸兔子 (rainbit-ye.github.io)</a>)：<br>$$<br>Attention(Q,K,V)&#x3D;softmax(\frac{QK^T}{\sqrt{d_k}})V<br>$$<br>在最原始的Transformer模型中，使用的是正弦三角函数的方法记录时序信息位置。在2018年 Shawet al. (2018) 对它进行优化，引入了相对位置进行表示，涉及到 $(H, L, Dh)$ 的单独相对位置嵌入$ Er$，它分别对位置$ iq $和$ jk $中的查询和键之间的每个可能的成对距离 $r &#x3D; jk − iq $进行嵌入，并将嵌入从$-L+1$开始排序，并为每个头进行单独学习。通过相对嵌入和查询产生的$S^{rel}$矩阵来调节每个头的注意力概率<br>$$<br>RelativeAttention&#x3D;softmax(\frac{QK^T+S^{rel}}{\sqrt{d_k}})V<br>$$</p>
</li>
</ul>
<h4 id="将-O-L-2D-优化为-O-LD"><a href="#将-O-L-2D-优化为-O-LD" class="headerlink" title="将$O(L^2D)$优化为$O(LD)$"></a>将$O(L^2D)$优化为$O(LD)$</h4><p>在Shawet所提出的方法基础上，放弃了头部指数。替换为形状为 $(L, L, Dh)$ 的中间量R，其中包含对应于所有键和查询之间的相对距离的嵌入。然后将 Q 重塑为$ (L, 1, Dh) $矢量，并且 $Srel &#x3D; QR^T$ 这会导致总空间复杂度为$ O(L^2D)$，从而限制其在长序列上的应用。并将将$ (iq , r) $索引矩阵转换为$ (iq , jk) $索引矩阵。行索引$ iq $保持不变，而列索引根据以下等式进行移动：$ jk &#x3D; r − (L − 1) + iq $。可以得知通过这样的操作可以使$O(L^2D)$优化为$O(LD)$</p>
<p>在这里贴上原文和原图以及翻译：</p>
<blockquote>
<p>Hence, we propose a “skewing” procedure to transform an absolute-by-relative$ (iq , r)$ indexed matrix into an absolute-by-absolute $(iq , jk) $indexed matrix. The row indices iq stay the same while the columns indices are shifted according to the following equation: $jk &#x3D; r − (L − 1) + iq$ . For example in Figure 1 the upper right green dot in position (0, 2) of $QE^{rT}$ after skewing has a column index of 2 − (3 − 1) + 0 &#x3D; 0, resulting in a position of (0, 0) in $S^{rel}$</p>
<p>We outline the steps illustrated in Figure 1 below.<br>\1. Pad a dummy column vector of length L before the leftmost column.<br>\2. Reshape the matrix to have shape $(L+1, L)$. (This step assumes NumPy-style row-major ordering.)<br>\3. Slice that matrix to retain only the last $ l $ rows and all the columns, resulting in a (L, L) matrix<br>again, but now absolute-by-absolute indexed, which is the $S^{rel} $that we need</p>
</blockquote>
<blockquote>
<p> 因此，我们提出了一种“倾斜”过程，将绝对相对$ (iq , r) $索引矩阵转换为绝对相对 $(iq , jk) $索引矩阵。行索引 $iq$ 保持不变，而列索引根据以下等式进行移动：$ jk &#x3D; r − (L − 1) + iq $。例如，在图 1 中$QE^{rT}$ 的位置 (0, 2) 中的右上角绿点在倾斜后的列索引为 2 − (3 − 1) + 0 &#x3D; 0，通过计算得到的位置为 (0, 0)的$S^{rel} $。我们概述了下面图 1 所示的步骤：</p>
<ol>
<li>在最左边的列之前填充长度为$ L $的虚拟列向量。</li>
<li>将矩阵重塑为形状 $(L+1, L)$。 （此步骤假定 NumPy 样式的行优先排序。）</li>
<li>对该矩阵进行切片以仅保留最后 $l$ 行和所有列，再次生成 (L, L) 矩阵，但现在是逐个绝对索引的，这就是我们需要的 $S^{rel}$</li>
</ol>
</blockquote>
<center>
<div style="display:inline-block"><img src="/../img/Trm/ecd70c6295a22d33a83fa3b4cae4c45.png" width="800" title="优化算法"></div>
</center>



<h4 id="相对局部注意力（本人翻译，原文为RELATIVE-LOCAL-ATTENTION）"><a href="#相对局部注意力（本人翻译，原文为RELATIVE-LOCAL-ATTENTION）" class="headerlink" title="相对局部注意力（本人翻译，原文为RELATIVE LOCAL ATTENTION）"></a>相对局部注意力（本人翻译，原文为RELATIVE LOCAL ATTENTION）</h4><p>对于超长序列来说，线性的Transformer对于记忆复杂性也还是不切实际。为了将相对注意力扩展到局部情况，文章进行了算法设计。</p>
<p>贴上原文，原图和翻译</p>
<blockquote>
<p>For very long sequences, the quadratic memory requirement of even baseline Transformer is impractical. Local attention has been used for example in Wikipedia and image generation (Liu et al., 2018; Parmar et al., 2018) by chunking the input sequence into non-overlapping blocks. Each block then attends to itself and the one before, as shown by the smaller thumbnail on the top right corner of Figure 2. To extend relative attention to the local case, we first note that the right block has the same configuration as in the global case (see Figure 1) but much smaller: $( \frac{L}M )^2$ (where M is the number of blocks, and N be the resulting block length) as opposed to L2. The left block is unmasked with relative indices running from -1 (top right) to$ -2N + 1 $(bottom left). Hence, the learned Er for the local case has shape $(2N − 1, N ).$<br>Similar to the global case, we first compute $QE^rT$ and then use the following procedure to skew it to have the same indexing as $QK^T$, as illustrated in Figure 2.<br>\1. Pad a dummy column vector of length N after the rightmost column.<br>\2. Flatten the matrix and then pad with a dummy row of length $N − 1$.<br>\3. Reshape the matrix to have shape$ (N + 1, 2N − 1)$.</p>
</blockquote>
<blockquote>
<p>对于很长的序列，即使是基于线性的 Transformer 的二次内存要求也是不切实际的。例如，局部注意力已通过将输入序列分块为不重叠的块去用于维基百科和图像生成（Liu et al., 2018；Parmar et al., 2018）</p>
<p>每个块都会关注本身和之前的块，如图 2 右上角的较小缩略图所示。为了将相对注意力扩展到本地情况，我们首先注意到右侧块具有与全局情况（参见图 1）但小得多：$ (\frac{L}M )^2$（其中 M 是块数，N 是结果块长度）。左侧块未进行Mask处理，相对索引从 -1（右上）到 -2N + 1（左下）。因此，本地情况下学习到的 $E^r$的形状为 (2N − 1, N )。与全局情况类似，我们首先计算 $QE^{rT}$，然后使用以下过程将其倾斜，使其具有与 $QE^{r}$ 相同的索引，如图 2 所示。</p>
<ol>
<li>在最右边的列后面填充长度为$ N $的虚拟列向量</li>
<li>展平矩阵，然后填充长度为$ N − 1 $的虚拟行</li>
<li>将矩阵重塑为形状$ (N + 1, 2N − 1)$</li>
<li>对该矩阵进行切片，仅保留前 N 行和后 N 列，从而得到$ (N, N ) $矩阵。</li>
</ol>
</blockquote>
<center>
<div style="display:inline-block"><img src="/../img/Trm/888a8428e80f56519a0cefbd4d6b395.png" width="800" title="流程"></div>
</center>
从文中指出，过程中计算中间张量 $R$ 的步骤是导致较高复杂度的主要原因。作者的主要想法是，所有在 $QR^T$ 中需要的项均已在 $QE^{rT}$中。因此，选择直接计算$QE^{rT}$，得到Query的绝对坐标嵌入与相对位置的内积矩阵；随后进行偏移变换（skew）的方式进行计算。


<h4 id="实验数据"><a href="#实验数据" class="headerlink" title="实验数据"></a>实验数据</h4><p>实验表明做以上修改后相对注意力机制比baseline Transformer</p>
<ul>
<li>极大的提高了负相对似然数</li>
<li>在全局层面上捕获时序</li>
<li>样本可以保持必要的时间&#x2F;声部网格结构</li>
</ul>
<p>同时作者也对绝对时值进行研究，并用Concat 操作来替代正弦曲线，也取得了一定优化效果</p>
<p>与LSTM相比，文章的模型更加稳定，不会出现像LSTM那样，一定时间后会出现乱码数据。而使用本文中提出的方法可以使其更稳定。</p>
<p>给定旋律进行伴奏创作的情况下，Music Transformer相比baseline也在NLL上有很大提升。</p>

                    
                      <div id="toc-article">
                        
  <div class="widget-wrap" id="toc-wrap">
    <h3 class="widget-title"><i class="fa fa-toc"></i> Contents</h3>
    <div class="widget">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-text">前言</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D"><span class="toc-text">介绍</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-text">模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%86-O-L-2D-%E4%BC%98%E5%8C%96%E4%B8%BA-O-LD"><span class="toc-text">将$O(L^2D)$优化为$O(LD)$</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%B8%E5%AF%B9%E5%B1%80%E9%83%A8%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%88%E6%9C%AC%E4%BA%BA%E7%BF%BB%E8%AF%91%EF%BC%8C%E5%8E%9F%E6%96%87%E4%B8%BARELATIVE-LOCAL-ATTENTION%EF%BC%89"><span class="toc-text">相对局部注意力（本人翻译，原文为RELATIVE LOCAL ATTENTION）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E6%95%B0%E6%8D%AE"><span class="toc-text">实验数据</span></a></li></ol></li></ol>
    </div>
  </div>


                      </div>
                      
                        
                            
          </div>
          <footer class="article-footer">
            
                  <div class="article-tag-wrap">
                    

                      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/" rel="tag">AI</a></li></ul>

                        


                  </div>
                  
                    
                      
<nav id="article-nav">
  
    <a href="/2024/03/10/Transformer%E5%88%9D%E6%AD%A5%E4%BA%86%E8%A7%A3/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">older</strong>
      <div class="article-nav-title">
        
          Transformer初步了解
        
      </div>
    </a>
  
  
    <a href="/2024/03/31/UnityShader%E6%B8%B2%E6%9F%93%E5%A4%A9%E7%A9%BA%E4%BA%91%E4%BD%93/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">newer</strong>
      <div class="article-nav-title">
        
          UnityShader渲染天空云体
        
      </div>
    </a>
  
</nav>

                        
                          
                            








                              
          </footer>
  </div>
</article></section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title"><i class="fa fa-posts"></i> Recent</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/03/31/UnityShader%E6%B8%B2%E6%9F%93%E5%A4%A9%E7%A9%BA%E4%BA%91%E4%BD%93/">UnityShader渲染天空云体</a>
          </li>
        
          <li>
            <a href="/2024/03/19/MUSIC-TRANSFORMER-%E8%A7%A3%E8%AF%BB/">MUSIC TRANSFORMER 解读</a>
          </li>
        
          <li>
            <a href="/2024/03/10/Transformer%E5%88%9D%E6%AD%A5%E4%BA%86%E8%A7%A3/">Transformer初步了解</a>
          </li>
        
          <li>
            <a href="/2024/03/02/Sobel%E5%AF%B9%E5%9B%BE%E7%89%87%E7%9A%84%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/">Sobel对图片的边缘检测</a>
          </li>
        
          <li>
            <a href="/2024/02/28/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%8C%AB%E7%8B%97%E8%AF%86%E5%88%AB/">卷积神经网络-猫狗识别</a>
          </li>
        
      </ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title"><i class="fa fa-tag"></i> Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/AI/" style="font-size: 20px;">AI</a> <a href="/tags/Art/" style="font-size: 10px;">Art</a> <a href="/tags/Unity/" style="font-size: 15px;">Unity</a> <a href="/tags/Unity-Shader/" style="font-size: 15px;">Unity Shader</a> <a href="/tags/%E8%99%9A%E6%8B%9F%E7%8E%B0%E5%AE%9E/" style="font-size: 10px;">虚拟现实</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" style="font-size: 10px;">计算机视觉</a>
    </div>
  </div>

  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title"><i class="fa fa-archive"></i> Archive</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/">2024年</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/">2023年</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/">2022年</a><span class="archive-list-count">4</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title"><i class="fa fa-tag"></i> Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI/" rel="tag">AI</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Art/" rel="tag">Art</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Unity/" rel="tag">Unity</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Unity-Shader/" rel="tag">Unity Shader</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%99%9A%E6%8B%9F%E7%8E%B0%E5%AE%9E/" rel="tag">虚拟现实</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" rel="tag">计算机视觉</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>


  
    

  
</aside>
        
      </div>
      <a id="totop" href="#top"></a>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      <p>
        <a href="/sitemap.xml">Site Map</a>
        <span> | </span><a href="/atom.xml">Subscribe to this site</a>
        <span> | </span><a href="/about/">Contact the blogger</a>
      </p>
      
        <p>
          <i class="fa fa-visitors"></i>
          <i id="busuanzi_container_site_uv"><i id="busuanzi_value_site_uv"></i></i>
          ，
          <i class="fa fa-views"></i>
          <i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>
        </p>
      
      <p>
        <span>Copyright &copy; 2024 Rainbit.</span>
        <span>Theme by <a href="https://github.com/chaooo/hexo-theme-BlueLake/" target="_blank">BlueLake.</a></span>
        <span>Powered by <a href="https://hexo.io/" target="_blank">Hexo.</a></span>
      </p>
    </div>
  </div>
</footer>

    </div>
  </div>
  
<script src="/js/jquery-3.4.1.min.js"></script>


<script src="/js/search.json.js"></script>


  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>






  
<script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  
  



  

  

  

  

  

  

  

  
  





  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>
  <script src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML" async type="text/javascript"></script>

</body>
</html>